## 最大似然估计(最小均方差)

我们常用MLE(maximum likelihood estimation)来估计一个统计模型的参数, 即,

$$ \hat{\theta} \triangleq arg \mathop{max}_{\theta}\log{p(\cal{D}|\theta)} \tag{7.4}$$

通常会假设训练数据是独立同分布的(independent & identically distributed, iid). 此时log似然可以写成如下形式:

$$ \ell(\theta) \triangleq \log(p(\cal{D}|\theta) = \sum_{i=1}^{N}\log{p(y_i|x_i, \theta)} \tag{7.5}$$

一般我们会等效地优化负log似然(negative log likeli-hood, NLL):

$$\text{NLL}(\theta) \triangleq - \sum_{i=1}^{N}\log{p(y_i|x_i, \theta)}\tag{7.6}$$

用NLL更为方便, 因为很多软件包都设计成最小化某个函数, 而不是最大化.

现在我们把MLE用到线性回归里. 我们把高斯分布的表达式套入, log似然函数就变成了:
$$
\begin{align}
\ell(\theta)&=\sum_{i=1}^{N}\log{\left[\left(\frac{1}{2\pi\sigma^2}\right)^{\frac12}\exp\left(-\frac1{2\sigma^2}(y_i-w^Tx_i)\right)^2\right]} \tag{7.7}\\
&=\frac{-1}{2\sigma^2}RSS(w)-\frac N2\log(2\pi\sigma^2) \tag{7.8}
\end{align}
$$
RSS就是平方和残差(residual sum of squares), 即

$$RSS(w)\triangleq\sum_{i=1}^N(y_i-w^Tx_i)^2\tag{7.9}$$

这个RSS也叫作平方误差(sum of squared errors)或SSE. SSE/N 叫做均方误差(mean squared error)或MSE. 它也可以写成残差向量的二阶范数的平方





